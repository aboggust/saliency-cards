{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Agnosticism \n",
    "\n",
    "*Code to create Figure 3c in [Saliency Cards: A Framework to Characterize and Compare\n",
    "Saliency Methods](https://arxiv.org/abs/2206.02958).*\n",
    "\n",
    "Model agnosticism measures how much access to the model a saliency method requires. Model agnostic methods treat the underlying model as a black box, relying only on its input and output. On the other hand, model-dependent methods require access to model internals.\n",
    "\n",
    "To illustrate model agnosticism, we show how [RISE](https://arxiv.org/pdf/1806.07421.pdf), a model agnostic method can be used to interpret two completely different model architectues: a CNN and a Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from interpretability_methods.rise import RISE\n",
    "from interpretability_methods.util import visualize_saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some user-specific paramters that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '~/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RISE for an MNIST CNN\n",
    "\n",
    "Using Pytorch we train a simple Convolutional Neural Network to classify MNIST images and apply the saliency method [RISE](https://arxiv.org/pdf/1806.07421.pdf) to interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the [MNIST dataset in PyTorch](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(DATA_DIR, train=True, download=True, \n",
    "                               transform=transform)\n",
    "test_dataset = datasets.MNIST(DATA_DIR, train=False, download=True, \n",
    "                              transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, \n",
    "                                           num_workers=1, pin_memory=True, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, \n",
    "                                          num_workers=1, pin_memory=True, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a CNN model, following the PyTorch basic [MNIST CNN example](https://github.com/pytorch/examples/tree/main/mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Simple CNN model.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn_model = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the training and testing procedures and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Performs one training epoch.\"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    \"\"\"Tests the model and prints the loss and accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adadelta(cnn_model.parameters(), lr=1.0)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.292426\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.134626\n",
      "\n",
      "Test set: Average loss: 0.0523, Accuracy: 9833/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.089992\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.063703\n",
      "\n",
      "Test set: Average loss: 0.0387, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.107298\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.053840\n",
      "\n",
      "Test set: Average loss: 0.0324, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.036134\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.047756\n",
      "\n",
      "Test set: Average loss: 0.0308, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.014154\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.074495\n",
      "\n",
      "Test set: Average loss: 0.0266, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.074979\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.008613\n",
      "\n",
      "Test set: Average loss: 0.0265, Accuracy: 9925/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.016624\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.012476\n",
      "\n",
      "Test set: Average loss: 0.0262, Accuracy: 9919/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.062606\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.015080\n",
      "\n",
      "Test set: Average loss: 0.0266, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.015776\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.003603\n",
      "\n",
      "Test set: Average loss: 0.0259, Accuracy: 9925/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.017419\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.006452\n",
      "\n",
      "Test set: Average loss: 0.0260, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.023314\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.002391\n",
      "\n",
      "Test set: Average loss: 0.0255, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.039718\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.083989\n",
      "\n",
      "Test set: Average loss: 0.0254, Accuracy: 9927/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.002157\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.005762\n",
      "\n",
      "Test set: Average loss: 0.0253, Accuracy: 9922/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.008362\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.022283\n",
      "\n",
      "Test set: Average loss: 0.0254, Accuracy: 9927/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 15):\n",
    "    train(cnn_model, device, train_loader, optimizer, epoch)\n",
    "    test(cnn_model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply RISE to MNIST examples and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, _ = test_dataset[0]\n",
    "image_size = image.shape[1:]\n",
    "image_batch = image.unsqueeze(0).to(device)\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rise_cnn = RISE(cnn_model, image_size, 1000)\n",
    "rise_cnn_saliency = rise_cnn.get_saliency(image_batch).squeeze(0)\n",
    "visualize_saliency(rise_cnn_saliency, images=image, scheme='heatmap', absolute_values=False, image_cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RISE for an MNIST Random Forest Classifier\n",
    "\n",
    "Now we do the same proceudre, but this time use SciKit Learn we train a Random Forest classifier on MNIST images and apply the saliency method [RISE](https://arxiv.org/pdf/1806.07421.pdf) to interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert the MNIST data to SciKit Learn format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pytorch_dataset(dataset):\n",
    "    \"\"\"Converts PyTorch Dataset object to SciKit Learn dataset format.\n",
    "    \n",
    "    Returns data, a 2D numpy array of the flattened dataset images, and targets,\n",
    "    a 1D numpy array of the dataset labels.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    for image, label in dataset:\n",
    "        flattened_image = image.detach().cpu().numpy().reshape(-1)\n",
    "        data.append(flattened_image)\n",
    "        targets.append(label)\n",
    "    return np.array(data), np.array(targets)\n",
    "\n",
    "train_data, train_targets = convert_pytorch_dataset(train_dataset)\n",
    "test_data, test_targets = convert_pytorch_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a [random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(train_data, train_targets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier acheives 96.95% accuracy on MNIST.\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = rf_model.score(test_data, test_targets)\n",
    "print(f'Random forest classifier acheives {test_accuracy:.2%} accuracy on MNIST.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run [RISE](https://arxiv.org/pdf/1806.07421.pdf) on the random forest classifier (a very different architecture than our prvious CNN model). This is only possible because RISE is model agnostic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model_wrapper(image_batch):\n",
    "    \"\"\"Wraps the SciKit random forest model to work like a PyTorch model for \n",
    "    integration with the interpretability methods package.\"\"\"\n",
    "    rf_image_batch = image_batch.flatten(start_dim=1).detach().cpu().numpy()\n",
    "    output = rf_model.predict_proba(rf_image_batch)\n",
    "    tensor_output = torch.from_numpy(output.astype(np.float32)).to(device)\n",
    "    return tensor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rise_rf = RISE(rf_model_wrapper, image_size, 1000)\n",
    "rise_rf_saliency = rise_rf.get_saliency(image_batch).squeeze(0)\n",
    "visualize_saliency(rise_rf_saliency, images=image, scheme='heatmap', absolute_values=False, image_cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
