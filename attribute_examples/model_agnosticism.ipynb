{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Agnosticism \n",
    "\n",
    "*Code to create Figure 3c in [Saliency Cards: A Framework to Characterize and Compare\n",
    "Saliency Methods](https://arxiv.org/abs/2206.02958).*\n",
    "\n",
    "Model agnosticism measures how much access to the model a saliency method requires. Model agnostic methods treat the underlying model as a black box, relying only on its input and output. On the other hand, model-dependent methods require access to model internals.\n",
    "\n",
    "To illustrate model agnosticism, we show how [RISE](https://arxiv.org/pdf/1806.07421.pdf), a model agnostic method can be used to interpret two completely different model architectues: a CNN and a Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from saliency_methods.rise import RISE\n",
    "from saliency_methods.util import visualize_saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some user-specific paramters that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '~/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RISE for an MNIST CNN\n",
    "\n",
    "Using Pytorch we train a simple Convolutional Neural Network to classify MNIST images and apply the saliency method [RISE](https://arxiv.org/pdf/1806.07421.pdf) to interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the [MNIST dataset in PyTorch](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(DATA_DIR, train=True, download=True, \n",
    "                               transform=transform)\n",
    "test_dataset = datasets.MNIST(DATA_DIR, train=False, download=True, \n",
    "                              transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, \n",
    "                                           num_workers=1, pin_memory=True, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, \n",
    "                                          num_workers=1, pin_memory=True, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a CNN model, following the PyTorch basic [MNIST CNN example](https://github.com/pytorch/examples/tree/main/mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Simple CNN model.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn_model = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the training and testing procedures and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Performs one training epoch.\"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    \"\"\"Tests the model and prints the loss and accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adadelta(cnn_model.parameters(), lr=1.0)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304075\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.290838\n",
      "\n",
      "Test set: Average loss: 0.0525, Accuracy: 9821/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.060676\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.025788\n",
      "\n",
      "Test set: Average loss: 0.0338, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.040387\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.040586\n",
      "\n",
      "Test set: Average loss: 0.0296, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.026051\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.065485\n",
      "\n",
      "Test set: Average loss: 0.0293, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.042140\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.018853\n",
      "\n",
      "Test set: Average loss: 0.0275, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.012586\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.020024\n",
      "\n",
      "Test set: Average loss: 0.0280, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.008461\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.006461\n",
      "\n",
      "Test set: Average loss: 0.0260, Accuracy: 9925/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.032008\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.009509\n",
      "\n",
      "Test set: Average loss: 0.0257, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002659\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.010081\n",
      "\n",
      "Test set: Average loss: 0.0262, Accuracy: 9929/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006740\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.082291\n",
      "\n",
      "Test set: Average loss: 0.0259, Accuracy: 9931/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.014944\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.010379\n",
      "\n",
      "Test set: Average loss: 0.0260, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.014561\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.002533\n",
      "\n",
      "Test set: Average loss: 0.0260, Accuracy: 9931/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.017396\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.042089\n",
      "\n",
      "Test set: Average loss: 0.0253, Accuracy: 9930/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.014415\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.004714\n",
      "\n",
      "Test set: Average loss: 0.0250, Accuracy: 9928/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 15):\n",
    "    train(cnn_model, device, train_loader, optimizer, epoch)\n",
    "    test(cnn_model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply RISE to MNIST examples and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, _ = test_dataset[0]\n",
    "image_size = image.shape[1:]\n",
    "image_batch = image.unsqueeze(0).to(device)\n",
    "display_batch = image_batch.detach().cpu().numpy()\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rise_cnn = RISE(cnn_model, image_size, 1000)\n",
    "rise_cnn_saliency = rise_cnn.get_saliency(image_batch).squeeze(0)\n",
    "visualize_saliency(rise_cnn_saliency, images=display_batch, scheme='heatmap', absolute_values=False, image_cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RISE for an MNIST Random Forest Classifier\n",
    "\n",
    "Now we do the same proceudre, but this time use SciKit Learn we train a Random Forest classifier on MNIST images and apply the saliency method [RISE](https://arxiv.org/pdf/1806.07421.pdf) to interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert the MNIST data to SciKit Learn format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pytorch_dataset(dataset):\n",
    "    \"\"\"Converts PyTorch Dataset object to SciKit Learn dataset format.\n",
    "    \n",
    "    Returns data, a 2D numpy array of the flattened dataset images, and targets,\n",
    "    a 1D numpy array of the dataset labels.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    for image, label in dataset:\n",
    "        flattened_image = image.detach().cpu().numpy().reshape(-1)\n",
    "        data.append(flattened_image)\n",
    "        targets.append(label)\n",
    "    return np.array(data), np.array(targets)\n",
    "\n",
    "train_data, train_targets = convert_pytorch_dataset(train_dataset)\n",
    "test_data, test_targets = convert_pytorch_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a [random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(train_data, train_targets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier acheives 97.04% accuracy on MNIST.\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = rf_model.score(test_data, test_targets)\n",
    "print(f'Random forest classifier acheives {test_accuracy:.2%} accuracy on MNIST.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run [RISE](https://arxiv.org/pdf/1806.07421.pdf) on the random forest classifier (a very different architecture than our prvious CNN model). This is only possible because RISE is model agnostic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model_wrapper(image_batch):\n",
    "    \"\"\"Wraps the SciKit random forest model to work like a PyTorch model for \n",
    "    integration with the interpretability methods package.\"\"\"\n",
    "    rf_image_batch = image_batch.flatten(start_dim=1).detach().cpu().numpy()\n",
    "    output = rf_model.predict_proba(rf_image_batch)\n",
    "    tensor_output = torch.from_numpy(output.astype(np.float32)).to(device)\n",
    "    return tensor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rise_rf = RISE(rf_model_wrapper, image_size, 1000)\n",
    "rise_rf_saliency = rise_rf.get_saliency(image_batch).squeeze(0)\n",
    "visualize_saliency(rise_rf_saliency, images=display_batch, scheme='heatmap', absolute_values=False, image_cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
